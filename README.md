# Modality Gap Visualization in Vision-Language Models (VLMs)
## Overview
This project aims to visualize and analyze the modality gap that exists in popular Vision-Language Models (VLMs), such as CLIP, BLIP, BLIP-2, SigLIP, and LLaVA. The modality gap refers to the differences in feature representations between visual and textual embeddings, which can impact model alignment and performance.

## Usage
### Clone the repository
git clone https://github.com/rajibrhasan/ModalityGap.git 

### Install required libraries
pip install -r requirements.txt

### Download Multimodal Dataset

### Run
bash scripts/llava.sh

